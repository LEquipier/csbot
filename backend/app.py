#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
T+7 é¥°å“äº¤æ˜“ç®¡ç†ç³»ç»Ÿ
åŸºäºFlaskçš„Webåº”ç”¨ï¼Œæä¾›æ•°æ®å¯è§†åŒ–ã€é€‰å“å»ºè®®ã€æŒä»“ç®¡ç†ç­‰åŠŸèƒ½
"""

import os
import sys
import json
import subprocess
from flask import Flask, render_template, jsonify, request
from flask_cors import CORS
import logging
from datetime import datetime, timedelta
from pathlib import Path
import pandas as pd
import glob

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from backend.CSQAQ import CsqaqClient
from backend.config import API_TOKEN

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# è·¯å¾„é…ç½®
BACKEND_DIR = Path(__file__).resolve().parent
MODEL_DIR = BACKEND_DIR / "Model"
DATASET_DIR = MODEL_DIR / "dataset"
RESULTS_DIR = MODEL_DIR / "results"
STATE_DIR = MODEL_DIR / "state"

# ç¡®ä¿ç›®å½•å­˜åœ¨
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
STATE_DIR.mkdir(parents=True, exist_ok=True)

def create_app():
    """åˆ›å»ºFlaskåº”ç”¨"""
    app = Flask(__name__, 
               template_folder='../frontend/templates',
               static_folder='../frontend/static')
    
    # é…ç½®åº”ç”¨
    app.config['SECRET_KEY'] = 't7-trading-system-2025'
    app.config['JSON_AS_ASCII'] = False
    
    # CORS
    CORS(app, resources={r"/api/*": {"origins": "*"}})
    
    return app

def create_client():
    """åˆ›å»ºCSQAQå®¢æˆ·ç«¯"""
    try:
        client = CsqaqClient(api_token=API_TOKEN)
        logger.info("âœ… CSQAQå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        return client
    except Exception as e:
        logger.error(f"âŒ CSQAQå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        return None

# åˆ›å»ºåº”ç”¨å’Œå®¢æˆ·ç«¯
app = create_app()
client = create_client()

# ==================== å·¥å…·å‡½æ•° ====================

def get_latest_model_result():
    """è·å–æœ€æ–°çš„æ¨¡å‹åˆ†æç»“æœ"""
    result_file = RESULTS_DIR / "realtime_reco.json"
    if result_file.exists():
        try:
            with open(result_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"è¯»å–æ¨¡å‹ç»“æœå¤±è´¥: {e}")
    return None

def get_model_history(days=7):
    """è·å–å†å²æ¨¡å‹åˆ†æç»“æœ"""
    history_dir = RESULTS_DIR / "history"
    if not history_dir.exists():
        return []
    
    try:
        from datetime import datetime, timedelta
        cutoff_date = datetime.now() - timedelta(days=days)
        
        history_files = []
        for history_file in history_dir.glob("reco_*.json"):
            try:
                # ä»æ–‡ä»¶åæå–æ—¶é—´æˆ³
                timestamp_str = history_file.stem.split("_", 1)[1]
                file_date = datetime.strptime(timestamp_str, "%Y%m%d_%H%M%S")
                
                if file_date >= cutoff_date:
                    with open(history_file, 'r', encoding='utf-8') as f:
                        result = json.load(f)
                        result['file_date'] = file_date.strftime("%Y-%m-%d %H:%M:%S")
                        history_files.append(result)
            except Exception as e:
                logger.error(f"è¯»å–å†å²æ–‡ä»¶ {history_file.name} å¤±è´¥: {e}")
        
        # æŒ‰æ—¶é—´å€’åºæ’åˆ—
        history_files.sort(key=lambda x: x['file_date'], reverse=True)
        return history_files
    except Exception as e:
        logger.error(f"è·å–å†å²è®°å½•å¤±è´¥: {e}")
        return []

def get_positions():
    """è·å–æŒä»“æ•°æ®"""
    positions_file = STATE_DIR / "positions.json"
    if positions_file.exists():
        try:
            with open(positions_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"è¯»å–æŒä»“æ•°æ®å¤±è´¥: {e}")
    return []

def save_positions(positions):
    """ä¿å­˜æŒä»“æ•°æ®"""
    positions_file = STATE_DIR / "positions.json"
    try:
        with open(positions_file, 'w', encoding='utf-8') as f:
            json.dump(positions, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        logger.error(f"ä¿å­˜æŒä»“æ•°æ®å¤±è´¥: {e}")
        return False

def run_model_analysis(mode="é€‚ä¸­", topk=8, lookback=336):
    """è¿è¡Œæ¨¡å‹åˆ†æ"""
    try:
        cmd = [
            sys.executable,
            str(MODEL_DIR / "model.py"),
            "--mode", mode,
            "--topk", str(topk),
            "--lookback", str(lookback)
        ]
        
        result = subprocess.run(
            cmd,
            cwd=str(BACKEND_DIR),
            capture_output=True,
            text=True,
            timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
        )
        
        if result.returncode == 0:
            return True, result.stdout
        else:
            return False, result.stderr
    except Exception as e:
        return False, str(e)

def clean_item_name(file_name):
    """æ¸…ç†ç‰©å“åç§°ï¼Œå»é™¤æ•°å­—å’Œç‰¹æ®Šå­—ç¬¦ï¼Œä½†ä¿ç•™Phaseåé¢çš„æ•°å­—"""
    import re
    
    # ç§»é™¤æ–‡ä»¶æ‰©å±•å
    name = file_name.replace('.csv', '')
    
    # æå–Phaseä¿¡æ¯
    phase_match = re.search(r'Phase(\d+)', name)
    phase_info = phase_match.group(0) if phase_match else ""
    
    # ç§»é™¤æ‰€æœ‰æ•°å­—
    name = re.sub(r'\d+', '', name)
    
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä½†ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€ç©ºæ ¼
    name = re.sub(r'[^\u4e00-\u9fff\w\s]', '', name)
    
    # æ¸…ç†å¤šä½™ç©ºæ ¼å’Œä¸‹åˆ’çº¿
    name = re.sub(r'\s+', ' ', name).strip()
    name = re.sub(r'_+', ' ', name).strip()
    
    # å¦‚æœæœ‰Phaseä¿¡æ¯ï¼Œæ·»åŠ åˆ°æœ«å°¾
    if phase_info:
        name = name.replace('Phase', '').strip()  # ç§»é™¤æ®‹ç•™çš„Phase
        name = f"{name} {phase_info}"
    
    # æœ€ç»ˆæ¸…ç†å¤šä½™ç©ºæ ¼
    name = re.sub(r'\s+', ' ', name).strip()
    
    return name

def get_dataset_stats():
    """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
    stats = {
        "total_files": 0,
        "total_records": 0,
        "knife_types": {},
        "latest_update": None
    }
    
    try:
        # éå†æ‰€æœ‰CSVæ–‡ä»¶
        csv_files = list(DATASET_DIR.rglob("*.csv"))
        stats["total_files"] = len(csv_files)
        
        latest_time = None
        
        for csv_file in csv_files:
            try:
                # è·å–åˆ€å‹ - ä»å®Œæ•´è·¯å¾„ä¸­æå–
                path_parts = csv_file.parts
                if len(path_parts) >= 4 and path_parts[-3] == "åŒ•é¦–":
                    knife_type = path_parts[-2]  # åŒ•é¦–ç›®å½•ä¸‹çš„å­ç›®å½•å
                else:
                    knife_type = csv_file.parent.name
                if knife_type not in stats["knife_types"]:
                    stats["knife_types"][knife_type] = {
                        "files": 0,
                        "records": 0
                    }
                
                stats["knife_types"][knife_type]["files"] += 1
                
                # è¯»å–CSVæ–‡ä»¶
                df = pd.read_csv(csv_file, encoding='utf-8')
                records = len(df)
                stats["knife_types"][knife_type]["records"] += records
                stats["total_records"] += records
                
                # æ›´æ–°æ—¶é—´
                if "date" in df.columns:
                    df["date"] = pd.to_datetime(df["date"], errors="coerce")
                    file_latest = df["date"].max()
                    if pd.notna(file_latest) and (latest_time is None or file_latest > latest_time):
                        latest_time = file_latest
                        
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶ {csv_file} å¤±è´¥: {e}")
                continue
        
        if latest_time:
            stats["latest_update"] = latest_time.strftime("%Y-%m-%d %H:%M")
            
    except Exception as e:
        logger.error(f"è·å–æ•°æ®é›†ç»Ÿè®¡å¤±è´¥: {e}")
    
    return stats

# ==================== è·¯ç”±å®šä¹‰ ====================

@app.route('/')
def index():
    """ä¸»é¡µ"""
    return render_template('index.html')

@app.route('/api/dashboard')
def dashboard_data():
    """è·å–ä»ªè¡¨æ¿æ•°æ®"""
    try:
        # è·å–æ¨¡å‹åˆ†æç»“æœ
        model_result = get_latest_model_result()
        
        # è·å–æ•°æ®é›†ç»Ÿè®¡
        dataset_stats = get_dataset_stats()
        
        # è·å–æŒä»“æ•°æ®
        positions = get_positions()
        
        return jsonify({
            'success': True,
            'model_result': model_result,
            'dataset_stats': dataset_stats,
            'positions': positions,
            'system_status': {
                'csqaq_client': client is not None,
                'last_update': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
        })
    except Exception as e:
        logger.error(f"è·å–ä»ªè¡¨æ¿æ•°æ®å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/model/analyze')
def run_analysis():
    """è¿è¡Œæ¨¡å‹åˆ†æ"""
    try:
        mode = request.args.get('mode', 'é€‚ä¸­')
        topk = int(request.args.get('topk', 8))
        lookback = int(request.args.get('lookback', 336))
        
        success, output = run_model_analysis(mode, topk, lookback)
        
        if success:
            # è·å–æœ€æ–°ç»“æœ
            result = get_latest_model_result()
            return jsonify({
                'success': True,
                'result': result,
                'output': output
            })
        else:
            return jsonify({
                'success': False,
                'error': output
            })
    except Exception as e:
        logger.error(f"è¿è¡Œæ¨¡å‹åˆ†æå¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/model/history')
def get_model_history_api():
    """è·å–å†å²æ¨¡å‹åˆ†æç»“æœ"""
    try:
        days = int(request.args.get('days', 7))
        history = get_model_history(days)
        return jsonify({
            'success': True,
            'history': history
        })
    except Exception as e:
        logger.error(f"è·å–å†å²è®°å½•å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/positions')
def get_positions_api():
    """è·å–æŒä»“æ•°æ®"""
    try:
        positions = get_positions()
        return jsonify({
            'success': True,
            'positions': positions
        })
    except Exception as e:
        logger.error(f"è·å–æŒä»“æ•°æ®å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/positions/record', methods=['POST'])
def record_buy():
    """è®°å½•ä¹°å…¥æ“ä½œ"""
    try:
        data = request.get_json()
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        required_fields = ['knife_type', 'item_file', 'platform', 'qty', 'price', 'buy_time']
        for field in required_fields:
            if field not in data:
                return jsonify({'success': False, 'error': f'ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}'})
        
        # æ„å»ºå‘½ä»¤
        cmd = [
            sys.executable,
            str(MODEL_DIR / "model.py"),
            "--record-buy",
            "--knife", data['knife_type'],
            "--item", data['item_file'],
            "--platform", data['platform'],
            "--qty", str(data['qty']),
            "--price", str(data['price']),
            "--time", data['buy_time']
        ]
        
        # æ‰§è¡Œå‘½ä»¤
        result = subprocess.run(
            cmd,
            cwd=str(BACKEND_DIR),
            capture_output=True,
            text=True,
            timeout=60
        )
        
        if result.returncode == 0:
            # é‡æ–°è·å–æŒä»“æ•°æ®
            positions = get_positions()
            return jsonify({
                'success': True,
                'message': 'ä¹°å…¥è®°å½•æˆåŠŸ',
                'positions': positions
            })
        else:
            return jsonify({
                'success': False,
                'error': result.stderr
            })
    except Exception as e:
        logger.error(f"è®°å½•ä¹°å…¥æ“ä½œå¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/dataset/items')
def get_dataset_items():
    """è·å–æ•°æ®é›†ä¸­çš„ç‰©å“åˆ—è¡¨"""
    try:
        knife_type = request.args.get('knife_type', '')
        # URLè§£ç 
        if knife_type:
            import urllib.parse
            knife_type = urllib.parse.unquote(knife_type)
        page = int(request.args.get('page', 1))
        size = int(request.args.get('size', 20))
        
        items = []
        
        # æ„å»ºæœç´¢è·¯å¾„
        logger.info(f"DATASET_DIR: {DATASET_DIR}")
        if knife_type:
            knife_dir = DATASET_DIR / "åŒ•é¦–" / knife_type
            logger.info(f"æœç´¢è·¯å¾„: {knife_dir}")
            logger.info(f"è·¯å¾„æ˜¯å¦å­˜åœ¨: {knife_dir.exists()}")
            csv_files = list(knife_dir.glob("*.csv")) if knife_dir.exists() else []
            logger.info(f"æœç´¢åˆ€å‹ {knife_type}ï¼Œæ‰¾åˆ° {len(csv_files)} ä¸ªæ–‡ä»¶")
            if len(csv_files) == 0:
                # åˆ—å‡ºåŒ•é¦–ç›®å½•ä¸‹çš„æ‰€æœ‰å­ç›®å½•
                dagger_dir = DATASET_DIR / "åŒ•é¦–"
                if dagger_dir.exists():
                    subdirs = [d.name for d in dagger_dir.iterdir() if d.is_dir()]
                    logger.info(f"åŒ•é¦–ç›®å½•ä¸‹çš„å­ç›®å½•: {subdirs}")
        else:
            csv_files = []
            for knife_dir in DATASET_DIR.glob("åŒ•é¦–/*"):
                if knife_dir.is_dir():
                    csv_files.extend(list(knife_dir.glob("*.csv")))
        
        # åˆ†é¡µ
        start_idx = (page - 1) * size
        end_idx = start_idx + size
        page_files = csv_files[start_idx:end_idx]
        
        for csv_file in page_files:
            try:
                # è¯»å–CSVæ–‡ä»¶
                try:
                    df = pd.read_csv(csv_file, encoding='utf-8')
                except UnicodeDecodeError:
                    df = pd.read_csv(csv_file, encoding='gbk')
                
                # è·å–æœ€æ–°æ•°æ®
                if len(df) > 0:
                    latest_row = df.iloc[-1]
                    
                    item_info = {
                        'file_name': csv_file.name,
                        'item_name': clean_item_name(csv_file.name),  # æ¸…ç†åçš„ç‰©å“åç§°
                        'knife_type': csv_file.parent.name,
                        'good_id': str(latest_row.get('good_id', '')),
                        'latest_date': str(latest_row.get('data', '')),  # æ³¨æ„ï¼šCSVä¸­æ˜¯'data'åˆ—
                        'buff_sell_price': float(latest_row.get('BUFF_sell_price', 0)),
                        'yyyp_sell_price': float(latest_row.get('YYYP_sell_price', 0)),
                        'buff_buy_price': float(latest_row.get('BUFF_buy_price', 0)),
                        'yyyp_buy_price': float(latest_row.get('YYYP_buy_price', 0)),
                        'buff_sell_num': int(latest_row.get('BUFF_sell_num', 0)),
                        'yyyp_sell_num': int(latest_row.get('YYYP_sell_num', 0)),
                        'total_records': int(len(df))
                    }
                    items.append(item_info)
                    
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶ {csv_file} å¤±è´¥: {e}")
                continue
        
        return jsonify({
            'success': True,
            'items': items,
            'total': len(csv_files),
            'page': page,
            'size': size
        })
    except Exception as e:
        logger.error(f"è·å–æ•°æ®é›†ç‰©å“åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/dataset/item/<path:file_name>')
def get_item_data(file_name):
    """è·å–ç‰¹å®šç‰©å“çš„è¯¦ç»†æ•°æ®"""
    try:
        knife_type = request.args.get('knife_type', '')
        # URLè§£ç 
        if knife_type:
            import urllib.parse
            knife_type = urllib.parse.unquote(knife_type)
        if not knife_type:
            return jsonify({'success': False, 'error': 'éœ€è¦æŒ‡å®šåˆ€å‹'})
        
        file_path = DATASET_DIR / "åŒ•é¦–" / knife_type / file_name
        
        if not file_path.exists():
            return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'})
        
        # è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(file_path, encoding='utf-8')
        
        # è½¬æ¢æ•°æ®æ ¼å¼
        data = []
        for _, row in df.iterrows():
            data.append({
                'date': row.get('date', ''),
                'buff_sell_price': row.get('BUFF_sell_price', 0),
                'yyyp_sell_price': row.get('YYYP_sell_price', 0),
                'buff_buy_price': row.get('BUFF_buy_price', 0),
                'yyyp_buy_price': row.get('YYYP_buy_price', 0),
                'buff_sell_num': row.get('BUFF_sell_num', 0),
                'yyyp_sell_num': row.get('YYYP_sell_num', 0),
                'buff_buy_num': row.get('BUFF_buy_num', 0),
                'yyyp_buy_num': row.get('YYYP_buy_num', 0),
                'buff_statistic': row.get('BUFF_statistic', 0),
                'yyyp_statistic': row.get('YYYP_statistic', 0)
            })
        
        return jsonify({
            'success': True,
            'data': data,
            'file_name': file_name,
            'knife_type': knife_type,
            'total_records': len(data)
        })
    except Exception as e:
        logger.error(f"è·å–ç‰©å“æ•°æ®å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/system/status')
def get_system_status():
    """è·å–ç³»ç»ŸçŠ¶æ€"""
    try:
        # æ£€æŸ¥auto_runè¿›ç¨‹
        auto_run_running = False
        try:
            result = subprocess.run(
                ['pgrep', '-f', 'auto_run.py'],
                capture_output=True,
                text=True
            )
            auto_run_running = result.returncode == 0
        except:
            pass
        
        # è·å–æœ€æ–°æ¨¡å‹ç»“æœæ—¶é—´
        model_result = get_latest_model_result()
        last_analysis = model_result.get('asof') if model_result else None
        
        # è·å–æ•°æ®é›†ç»Ÿè®¡
        dataset_stats = get_dataset_stats()
        
        return jsonify({
            'success': True,
            'status': {
                'csqaq_client': client is not None,
                'auto_run_running': auto_run_running,
                'last_analysis': last_analysis,
                'dataset_stats': dataset_stats,
                'system_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
        })
    except Exception as e:
        logger.error(f"è·å–ç³»ç»ŸçŠ¶æ€å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/system/start_auto_run')
def start_auto_run():
    """å¯åŠ¨è‡ªåŠ¨è¿è¡ŒæœåŠ¡"""
    try:
        cmd = [
            sys.executable,
            str(BACKEND_DIR / "auto_run.py"),
            "--daemon"
        ]
        
        # åœ¨åå°å¯åŠ¨
        subprocess.Popen(cmd, cwd=str(BACKEND_DIR))
        
        return jsonify({
            'success': True,
            'message': 'è‡ªåŠ¨è¿è¡ŒæœåŠ¡å·²å¯åŠ¨'
        })
    except Exception as e:
        logger.error(f"å¯åŠ¨è‡ªåŠ¨è¿è¡ŒæœåŠ¡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)})

# ==================== é”™è¯¯å¤„ç† ====================

@app.errorhandler(404)
def not_found(error):
    """404é”™è¯¯å¤„ç†"""
    return jsonify({'success': False, 'error': 'æ¥å£ä¸å­˜åœ¨'}), 404

@app.errorhandler(500)
def internal_error(error):
    """500é”™è¯¯å¤„ç†"""
    logger.error(f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {error}")
    return jsonify({'success': False, 'error': 'æœåŠ¡å™¨å†…éƒ¨é”™è¯¯'}), 500

# ==================== åº”ç”¨å¯åŠ¨ ====================

if __name__ == '__main__':
    print("ğŸš€ å¯åŠ¨T+7é¥°å“äº¤æ˜“ç®¡ç†ç³»ç»Ÿ...")
    print("ğŸ“Š è®¿é—®åœ°å€: http://localhost:5050")
    print("ğŸ”§ è°ƒè¯•æ¨¡å¼: å·²å¯ç”¨")
    app.run(host='0.0.0.0', port=5050, debug=True)
